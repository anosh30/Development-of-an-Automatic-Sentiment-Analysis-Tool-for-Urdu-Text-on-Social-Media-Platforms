{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa0c92d-3f91-48cf-bb1a-49dc43534b36",
   "metadata": {},
   "source": [
    "<h1> 21F-9065 Anosh Tariq NLP ASS#1 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a677d-635c-48e0-88cb-bbe962c4b8a0",
   "metadata": {},
   "source": [
    "<h1> PHASE 1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7380ecf2-ec78-4129-af8f-1c90e547b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  urdu_text   preprocessed_text\n",
      "0  ÛŒÛ Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Ø§ Ø¬Ù…Ù„Û ÛÛ’ ğŸ˜Š  Ù…Ø«Ø§Ù„ Ø¬Ù…Ù„Û positive\n",
      "2         Ù…Ø¬Ú¾Û’ ÛŒÛ Ø¨Ø±Ø§ Ù„Ú¯Ø§ ğŸ˜¢   Ù…Ø¬Ú¾Û’ Ù„Ú¯Ø§ negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Step 1: Define a list of Urdu stopwords\n",
    "urdu_stopwords = set([\n",
    "    \"Ø§ÙˆØ±\", \"ÛŒÛ\", \"Ú©Û\", \"Ú©Ø§\", \"Ú©ÛŒ\", \"ÛÛ’\", \"Ú©Ùˆ\", \"Ù…ÛŒÚº\", \"Ø³Û’\", \"ØªÚ¾Ø§\", \"ÛÙˆÚº\", \"ÛÛŒÚº\",\n",
    "    \"ØªÚ¾ÛŒ\", \"ØªÚ¾Ø§\", \"Ø§ÛŒÚ©\", \"Ù„ÛŒÛ’\", \"Ù†ÛÛŒÚº\", \"Ø¨Ø±Ø§\", \"Ø§Ú†Ú¾Ø§\", \"ØªÙ…\", \"Ú©Ú†Ú¾\", \"Ú©ÙˆÙ†\", \"Ú©ÛŒØ§\"\n",
    "])\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in urdu_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 2: Handle Punctuation, Emojis, and Hashtags\n",
    "\n",
    "# Emoji dictionary to map sentiment\n",
    "emoji_sentiment = {\n",
    "    \"ğŸ˜‚\": \"positive\", \"ğŸ¤£\": \"positive\", \"ğŸ˜Š\": \"positive\", \"ğŸ˜\": \"positive\", \"ğŸ˜\": \"positive\",\n",
    "    \"ğŸ˜¢\": \"negative\", \"ğŸ˜ \": \"negative\", \"ğŸ˜¡\": \"negative\", \"ğŸ˜­\": \"negative\", \"ğŸ˜’\": \"negative\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", '', text)\n",
    "    \n",
    "    # Replace emojis with sentiment\n",
    "    for emoji, sentiment in emoji_sentiment.items():\n",
    "        text = text.replace(emoji, sentiment)\n",
    "    \n",
    "    # Remove punctuation except Urdu script characters\n",
    "    urdu_punctuation = string.punctuation + \"ØŸ\"\n",
    "    text = text.translate(str.maketrans('', '', urdu_punctuation))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 3: Filter short conversations (less than 3 words)\n",
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    return len(words) >= 3\n",
    "\n",
    "# Sample data to simulate your dataset (replace with your actual data)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        'ÛŒÛ Ø§ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Ø§ Ø¬Ù…Ù„Û ÛÛ’ ğŸ˜Š',\n",
    "        'Ú©ÛŒØ§ ØªÙ… Ù†Û’ ÛŒÛ Ø³Ù†Ø§ ÛÛ’ØŸ',\n",
    "        'Ù…Ø¬Ú¾Û’ ÛŒÛ Ø¨Ø±Ø§ Ù„Ú¯Ø§ ğŸ˜¢',\n",
    "        'ÛŒÛ Ø§ÛŒÚ© Ø§ÙˆØ± Ø¬Ù…Ù„Û ÛÛ’ #Hashtag',\n",
    "        'https://example.com ÛŒÛ Ø§ÛŒÚ© Ù„Ù†Ú© ÛÛ’'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame (replace this with pd.read_csv or your actual data loading method)\n",
    "urdu_data_cleaned = pd.DataFrame(data)\n",
    "\n",
    "# Apply the preprocessing steps to the dataset\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['urdu_text'].apply(preprocess_text)\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['preprocessed_text'].apply(remove_stopwords)\n",
    "urdu_data_cleaned = urdu_data_cleaned[urdu_data_cleaned['preprocessed_text'].apply(filter_short_posts)]\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "print(urdu_data_cleaned[['urdu_text', 'preprocessed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400b8c4-a88d-4a17-a540-9868d32129e1",
   "metadata": {},
   "source": [
    "<h1> PHASE 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73bccaf-207c-424e-bfec-20c1f0217592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...</td>\n",
       "      <td>positivepositivepositive ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...</td>\n",
       "      <td>Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "      <td>Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...</td>\n",
       "      <td>Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ Ø­Ø§Ù…...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥</td>\n",
       "      <td>Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ğŸ’”ğŸ”¥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  ğŸ¤£ğŸ˜‚ğŸ˜‚ ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ÙØ³Ø§Ø¯Ù† Ù¹Ú¾ÛŒÚ© ÛÛ’ Ú©ÙˆØ¬ÛŒ Ù†Û...   \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ù…ÛŒÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ...   \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
       "4   `` Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ Ù…ÛŒÚº ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ...   \n",
       "5        Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ÛÛŒÚº ğŸ’”ğŸ”¥   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  positivepositivepositive ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ...  \n",
       "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº ...  \n",
       "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...  \n",
       "4  Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ Ø­Ø§Ù…...  \n",
       "5            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ğŸ’”ğŸ”¥  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "urdu_data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Define a list of Urdu stopwords\n",
    "urdu_stopwords = set([\n",
    "    \"Ø§ÙˆØ±\", \"ÛŒÛ\", \"Ú©Û\", \"Ú©Ø§\", \"Ú©ÛŒ\", \"ÛÛ’\", \"Ú©Ùˆ\", \"Ù…ÛŒÚº\", \"Ø³Û’\", \"ØªÚ¾Ø§\", \"ÛÙˆÚº\", \"ÛÛŒÚº\",\n",
    "    \"ØªÚ¾ÛŒ\", \"ØªÚ¾Ø§\", \"Ø§ÛŒÚ©\", \"Ù„ÛŒÛ’\", \"Ù†ÛÛŒÚº\", \"Ø¨Ø±Ø§\", \"Ø§Ú†Ú¾Ø§\", \"ØªÙ…\", \"Ú©Ú†Ú¾\", \"Ú©ÙˆÙ†\", \"Ú©ÛŒØ§\"\n",
    "])\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in urdu_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 2: Handle Punctuation, Emojis, and Hashtags\n",
    "\n",
    "# Emoji dictionary to map sentiment\n",
    "emoji_sentiment = {\n",
    "    \"ğŸ˜‚\": \"positive\", \"ğŸ¤£\": \"positive\", \"ğŸ˜Š\": \"positive\", \"ğŸ˜\": \"positive\", \"ğŸ˜\": \"positive\",\n",
    "    \"ğŸ˜¢\": \"negative\", \"ğŸ˜ \": \"negative\", \"ğŸ˜¡\": \"negative\", \"ğŸ˜­\": \"negative\", \"ğŸ˜’\": \"negative\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", '', text)\n",
    "    \n",
    "    # Replace emojis with sentiment\n",
    "    for emoji, sentiment in emoji_sentiment.items():\n",
    "        text = text.replace(emoji, sentiment)\n",
    "    \n",
    "    # Remove punctuation except Urdu script characters\n",
    "    urdu_punctuation = string.punctuation + \"ØŸ\"\n",
    "    text = text.translate(str.maketrans('', '', urdu_punctuation))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 3: Filter short conversations (less than 3 words)\n",
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    return len(words) >= 3\n",
    "\n",
    "# Clean the dataset and apply preprocessing\n",
    "urdu_data_cleaned = urdu_data[['urdu_text', 'is_sarcastic']].dropna()\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['urdu_text'].apply(preprocess_text)\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['preprocessed_text'].apply(remove_stopwords)\n",
    "urdu_data_cleaned = urdu_data_cleaned[urdu_data_cleaned['preprocessed_text'].apply(filter_short_posts)]\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "urdu_data_cleaned[['urdu_text', 'preprocessed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502795bb-1e9c-4f74-a40b-a723319f8be3",
   "metadata": {},
   "source": [
    "<h1> PHASE 3 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94e995-42fc-4b6e-bdf4-2133746191dc",
   "metadata": {},
   "source": [
    "<h2> TOKENIZATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e90c528-8603-4e36-8a05-a8dadcad7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  positivepositivepositive ÛÙˆ Ù„ÛŒÙ†Û’ Ø¯Û’ Ù…ÛŒØ±ÛŒ Ø´Ø§Ø¯ÛŒ ...   \n",
      "1  Ú†Ù„ Ù…ÛÙ…Ø§Ù†ÙˆÚº Ú©Ú¾Ø§Ù†Ø§ Ø³Ø±Ùˆ Ú©Ø± Ú†Ú‘ÛŒÙ„ Ú†Ø§Ú†ÛŒ Ù†ÙˆÚº Ø¯Ø³Ø¯ÛŒ Ø¢Úº ...   \n",
      "2  Ú©Ø§Ù…Ø±Ø§Ù† Ø®Ø§Ù† Ø¢Ù¾Ú©ÛŒ Ø¯Ù† Ø¨Ú¾Ø±ÛŒÛ Ø²Ù…Û Ø¯Ø§Ø±ÛŒ Ù„Ú¯Ø§Ø¦ÛŒ Ú¯Ø¦ÛŒ Ø§Ù¾...   \n",
      "4  Ù…Ø±Ø§Ø¯ Ø¹Ù„ÛŒ Ø´Ø§Û Ú©Û’ Ø¨Ú¾ÛŒØ³ ÚˆÛŒ Ø¬ÛŒ Ø¢Ø¦ÛŒ Ø§ÛŒØ³ Ø¢Ø¦ÛŒ ØªÚ¾Û’ Ø­Ø§Ù…...   \n",
      "5            Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÛŒ Ø§Ú©Ø«Ø± Ù‚Ø§ØªÙ„ Ø§Ø¹ØªØ¨Ø§Ø± ÛÙˆØªÛ’ ğŸ’”ğŸ”¥   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [positivepositivepositive, ÛÙˆ, Ù„ÛŒÙ†Û’, Ø¯Û’, Ù…ÛŒØ±ÛŒ,...  \n",
      "1  [Ú†Ù„, Ù…ÛÙ…Ø§Ù†ÙˆÚº, Ú©Ú¾Ø§Ù†Ø§, Ø³Ø±Ùˆ, Ú©Ø±, Ú†Ú‘ÛŒÙ„, Ú†Ø§Ú†ÛŒ, Ù†ÙˆÚº,...  \n",
      "2  [Ú©Ø§Ù…Ø±Ø§Ù†, Ø®Ø§Ù†, Ø¢Ù¾Ú©ÛŒ, Ø¯Ù†, Ø¨Ú¾Ø±ÛŒÛ, Ø²Ù…Û, Ø¯Ø§Ø±ÛŒ, Ù„Ú¯Ø§Ø¦...  \n",
      "4  [Ù…Ø±Ø§Ø¯, Ø¹Ù„ÛŒ, Ø´Ø§Û, Ú©Û’, Ø¨Ú¾ÛŒØ³, ÚˆÛŒ, Ø¬ÛŒ, Ø¢Ø¦ÛŒ, Ø§ÛŒØ³, Ø¢...  \n",
      "5       [Ù‚Ø§Ø¨Ù„, Ø§Ø¹ØªØ¨Ø§Ø±, ÛÛŒ, Ø§Ú©Ø«Ø±, Ù‚Ø§ØªÙ„, Ø§Ø¹ØªØ¨Ø§Ø±, ÛÙˆØªÛ’]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenization function for Urdu text\n",
    "def urdu_tokenizer(text):\n",
    "    # Tokenize based on spaces and punctuation\n",
    "    tokens = re.findall(r'\\b[\\wØ€-Û¿]+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to your dataset\n",
    "urdu_data_cleaned['tokenized_text'] = urdu_data_cleaned['preprocessed_text'].apply(urdu_tokenizer)\n",
    "\n",
    "# Display tokenized version of some Urdu social media posts\n",
    "print(urdu_data_cleaned[['preprocessed_text', 'tokenized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4829461-d930-496c-9c5e-7497d6f3da02",
   "metadata": {},
   "source": [
    "<h1> TF-IDF </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a20781f-a889-4565-845f-8e2763e806c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  tfidf_score\n",
      "22153        Ú©Û’   560.508533\n",
      "6956         ØªÙˆ   462.573622\n",
      "1447   positive   411.428428\n",
      "6139        Ø¨Ú¾ÛŒ   408.940076\n",
      "17118        Ù†Û’   366.229613\n",
      "20807        Ú©Ø±   343.570984\n",
      "23387        ÛÙˆ   307.013534\n",
      "23660        ÛÛŒ   283.271429\n",
      "18495        Ù¾Ø±   283.140206\n",
      "2480         Ø¢Ù¾   275.295158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a custom tokenizer using the previously defined function\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=urdu_tokenizer)\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(urdu_data_cleaned['preprocessed_text'])\n",
    "\n",
    "# Get feature names (i.e., the words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the TF-IDF scores\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Create a DataFrame with words and their TF-IDF scores\n",
    "tfidf_df = pd.DataFrame({'word': feature_names, 'tfidf_score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by TF-IDF scores in descending order\n",
    "tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773185d1-b0b3-4e70-9081-1ceded2817cf",
   "metadata": {},
   "source": [
    "<h1> WORD2VEC </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01bdfe1-7090-4c15-892e-a4f5db2b149c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconll\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoNLL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding: utf8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Pipeline module\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\core.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NormalizeParser\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizeParser\n\u001b[0;32m     10\u001b[0m NORMALIZE: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m TOKENIZE: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\parsers\\tokenize.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconll\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_tokenizer, word_tokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenizeParser\u001b[39;00m(Parser):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding: utf8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTokenization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m==============\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03msentence and word tokenization.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_tokenizer, word_tokenizer\n\u001b[0;32m     25\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\tokenizer.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _generate_sentences\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_model_exist, _preprocess_sentence, _retrieve_words, _load_model\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_PATH, VOCAB_PATH\n\u001b[0;32m     12\u001b[0m _WORD_TOKENIZER_MODEL, _CHAR2IDX, _IDX2CHAR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\keras_tokenizer.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_vocab\u001b[39m(vocab_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming tokenized data is available in the column 'tokenized_text'\n",
    "# Tokenized data should be a list of lists, where each sublist is a tokenized sentence\n",
    "tokenized_data = urdu_data_cleaned['tokenized_text'].tolist()\n",
    "\n",
    "# Step 1: Train Word2Vec model on tokenized data\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 2: Find the top 5 words most similar to 'Ø§Ú†Ú¾Ø§' (good)\n",
    "try:\n",
    "    similar_words = word2vec_model.wv.most_similar('Ø§Ú†Ú¾Ø§', topn=5)\n",
    "    # Display the top 5 similar words\n",
    "    print(\"Top 5 words similar to 'Ø§Ú†Ú¾Ø§':\", similar_words)\n",
    "except KeyError:\n",
    "    print(\"'Ø§Ú†Ú¾Ø§' not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d32f6-3a1e-4d01-84b8-dac7b7ce7325",
   "metadata": {},
   "source": [
    "<h1> PHASE 4 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00285f70-ac5e-4f91-bcd2-aa64f1c3e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c7f4fd-1d92-4c4e-9dd1-17751837a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Bigrams and their Frequencies:\n",
      "ÛŒÛ Ø§ÛŒÚ©: 2\n",
      "Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹: 2\n",
      "Ù¾ÙˆØ³Ù¹ ÛÛ’: 2\n",
      "Ø§ÛŒÚ© Ù¹ÛŒØ³Ù¹: 1\n",
      "Ø§ÛŒÚ© Ø§ÙˆØ±: 1\n",
      "Ø§ÙˆØ± Ù¹ÛŒØ³Ù¹: 1\n",
      "ÛÙ… Ù†Ú¯Ø±Ø§Ù…: 1\n",
      "Ù†Ú¯Ø±Ø§Ù… Ú©Û’: 1\n",
      "Ú©Û’ ØªØ¬Ø²ÛŒÛ’: 1\n",
      "ØªØ¬Ø²ÛŒÛ’ Ú©Ø§: 1\n",
      "\n",
      "Top 10 Trigrams and their Frequencies:\n",
      "Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹ ÛÛ’: 2\n",
      "ÛŒÛ Ø§ÛŒÚ© Ù¹ÛŒØ³Ù¹: 1\n",
      "Ø§ÛŒÚ© Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹: 1\n",
      "ÛŒÛ Ø§ÛŒÚ© Ø§ÙˆØ±: 1\n",
      "Ø§ÛŒÚ© Ø§ÙˆØ± Ù¹ÛŒØ³Ù¹: 1\n",
      "Ø§ÙˆØ± Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹: 1\n",
      "ÛÙ… Ù†Ú¯Ø±Ø§Ù… Ú©Û’: 1\n",
      "Ù†Ú¯Ø±Ø§Ù… Ú©Û’ ØªØ¬Ø²ÛŒÛ’: 1\n",
      "Ú©Û’ ØªØ¬Ø²ÛŒÛ’ Ú©Ø§: 1\n",
      "ØªØ¬Ø²ÛŒÛ’ Ú©Ø§ Ø§Ù…ØªØ­Ø§Ù†: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\intelcomputer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure that you have downloaded the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample dataset (Replace this with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹ ÛÛ’\",\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ø§ÙˆØ± Ù¹ÛŒØ³Ù¹ Ù¾ÙˆØ³Ù¹ ÛÛ’\",\n",
    "        \"ÛÙ… Ù†Ú¯Ø±Ø§Ù… Ú©Û’ ØªØ¬Ø²ÛŒÛ’ Ú©Ø§ Ø§Ù…ØªØ­Ø§Ù† Ù„Û’ Ø±ÛÛ’ ÛÛŒÚº\",\n",
    "        \"ÛŒÛ ØªØ¬Ø²ÛŒÛ Ø§Ø±Ø¯Ùˆ Ù…ØªÙ† Ú©Û’ Ù„ÛŒÛ’ ÛÛ’\"\n",
    "    ]\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Tokenization function for Urdu text\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Create lists to store unigrams, bigrams, and trigrams\n",
    "unigrams = []\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "\n",
    "# Process each document\n",
    "for text in urdu_data['urdu_text']:\n",
    "    tokens = tokenize(text)\n",
    "    unigrams.extend(tokens)\n",
    "    bigrams.extend(list(ngrams(tokens, 2)))\n",
    "    trigrams.extend(list(ngrams(tokens, 3)))\n",
    "\n",
    "# Count frequencies of unigrams, bigrams, and trigrams\n",
    "unigram_freq = Counter(unigrams)\n",
    "bigram_freq = Counter(bigrams)\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# Get the top 10 most common bigrams and trigrams\n",
    "top_bigrams = bigram_freq.most_common(10)\n",
    "top_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Bigrams and their Frequencies:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams and their Frequencies:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3563100-d3c8-421d-9dd6-5a2079901fd3",
   "metadata": {},
   "source": [
    "<h1> PHASE 5 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "752f0d21-902d-4055-99de-2d53744df323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f15e3f-1cea-4ce7-840a-6267661f879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "\n",
    "# Sample Urdu stopwords (You can expand this list)\n",
    "urdu_stopwords = set([\n",
    "    'Ú©ÛŒ', 'ÛÛ’', 'Ø§ÙˆØ±', 'Ù…ÛŒÚº', 'Ù¾Ø±', 'Ù†Û’', 'Ú©Ùˆ', 'Ú©ÛŒÙˆÚº', 'ØªÚ¾Ø§', 'ØªÚ¾ÛŒ', \n",
    "    'ÛŒÛ', 'ÙˆÛ', 'Ø³Ø¨', 'Ú©ÛŒØ§', 'Ø¬Ø¨', 'ØªÚ©', 'ÛÙ…', 'Ù…ÛŒØ±ÛŒ', 'Ù…Ø¬Ú¾Û’', 'ÛÙˆÚº'\n",
    "])\n",
    "\n",
    "# Sample dataset (Replace this with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ù…Ø«Ø¨Øª Ù¹ÛŒØ³Ù¹ ÛÛ’\",\n",
    "        \"ÛŒÛ Ù…Ù†ÙÛŒ Ø§Ø«Ø±Ø§Øª ÛÛŒÚº\",\n",
    "        \"Ù…ÛŒÚº Ø¨ÛØª Ø®ÙˆØ´ ÛÙˆÚº\",\n",
    "        \"ÛŒÛ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’\",\n",
    "        \"Ù…Ø¬Ú¾Û’ ÛŒÛ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"ÛŒÛ Ø¨ÛŒÚ©Ø§Ø± ÛÛ’\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess text (remove stopwords)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction using Tf-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Building - Using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef653924-ce9d-4124-8552-85a0c2b842d1",
   "metadata": {},
   "source": [
    "<h1> PHASE 6 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a595a6e4-aeda-4454-b85d-e274aee63b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 1.00\n",
      "F1 Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted  positive\n",
      "Actual             \n",
      "negative          1\n",
      "positive          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Sample Urdu dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ù…Ø«Ø¨Øª Ù¹ÛŒØ³Ù¹ ÛÛ’\",\n",
    "        \"ÛŒÛ Ù…Ù†ÙÛŒ Ø§Ø«Ø±Ø§Øª ÛÛŒÚº\",\n",
    "        \"Ù…ÛŒÚº Ø¨ÛØª Ø®ÙˆØ´ ÛÙˆÚº\",\n",
    "        \"ÛŒÛ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’\",\n",
    "        \"Ù…Ø¬Ú¾Û’ ÛŒÛ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"ÛŒÛ Ø¨ÛŒÚ©Ø§Ø± ÛÛ’\",\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ø­ÛŒØ±Øª Ø§Ù†Ú¯ÛŒØ² Ú©Ø§Ù… ÛÛ’\",\n",
    "        \"ÛŒÛ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ø¶Ø­Ú©Û Ø®ÛŒØ² ÛÛ’\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess text (manual stopwords as before)\n",
    "urdu_stopwords = set([...])  # Fill in with the previously defined stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean the text\n",
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction using Tf-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n",
    "\n",
    "# Model Building - Using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_recall = recall_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_f1 = f1_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "\n",
    "# Display the evaluation metrics for validation\n",
    "print(\"Validation Set Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Precision: {val_precision:.2f}\")\n",
    "print(f\"Recall: {val_recall:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Analyze predictions\n",
    "confusion_matrix = pd.crosstab(y_val, y_val_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf0873-deb2-46cd-ba67-432defe6ee9c",
   "metadata": {},
   "source": [
    "<h1> DESCRIPTION </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbbe887f-15c4-4849-bc78-e5ddc709aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f2dc9c-b9f7-43a3-9757-f0ebbe62e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ù…Ø«Ø¨Øª Ù¹ÛŒØ³Ù¹ ÛÛ’\",\n",
    "        \"ÛŒÛ Ù…Ù†ÙÛŒ Ø§Ø«Ø±Ø§Øª ÛÛŒÚº\",\n",
    "        \"Ù…ÛŒÚº Ø¨ÛØª Ø®ÙˆØ´ ÛÙˆÚº\",\n",
    "        \"ÛŒÛ Ø¨ÛØª Ø¨Ø±Ø§ ÛÛ’\",\n",
    "        \"Ù…Ø¬Ú¾Û’ ÛŒÛ Ù¾Ø³Ù†Ø¯ ÛÛ’\",\n",
    "        \"ÛŒÛ Ø¨ÛŒÚ©Ø§Ø± ÛÛ’\",\n",
    "        \"ÛŒÛ Ø§ÛŒÚ© Ø­ÛŒØ±Øª Ø§Ù†Ú¯ÛŒØ² Ú©Ø§Ù… ÛÛ’\",\n",
    "        \"ÛŒÛ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ø¶Ø­Ú©Û Ø®ÛŒØ² ÛÛ’\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a49bb5ac-0325-4cdb-b251-534de264c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_stopwords = set([...])  # Fill in with the previously defined stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed881665-6074-4128-af3e-0b2750e8c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06a37e12-25e2-417a-8d19-c070623b52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69e07c72-8782-4f7a-b4b2-0fdd443ebedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eadd58b0-d473-4047-9360-9739d4404441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e67fc266-a38a-4569-8755-2a5e1254d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 1.00\n",
      "F1 Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_recall = recall_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_f1 = f1_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "\n",
    "print(\"Validation Set Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Precision: {val_precision:.2f}\")\n",
    "print(f\"Recall: {val_recall:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3145e065-e7d0-410e-8180-661b3893dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "Predicted  positive\n",
      "Actual             \n",
      "negative          1\n",
      "positive          1\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(y_val, y_val_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1939209-ffc7-4e9b-92bd-9809171572ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
