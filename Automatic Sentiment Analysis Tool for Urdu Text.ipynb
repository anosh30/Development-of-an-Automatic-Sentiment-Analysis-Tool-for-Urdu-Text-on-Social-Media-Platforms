{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa0c92d-3f91-48cf-bb1a-49dc43534b36",
   "metadata": {},
   "source": [
    "<h1> 21F-9065 Anosh Tariq NLP ASS#1 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a677d-635c-48e0-88cb-bbe962c4b8a0",
   "metadata": {},
   "source": [
    "<h1> PHASE 1 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7380ecf2-ec78-4129-af8f-1c90e547b548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  urdu_text   preprocessed_text\n",
      "0  یہ ایک مثال کا جملہ ہے 😊  مثال جملہ positive\n",
      "2         مجھے یہ برا لگا 😢   مجھے لگا negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Step 1: Define a list of Urdu stopwords\n",
    "urdu_stopwords = set([\n",
    "    \"اور\", \"یہ\", \"کہ\", \"کا\", \"کی\", \"ہے\", \"کو\", \"میں\", \"سے\", \"تھا\", \"ہوں\", \"ہیں\",\n",
    "    \"تھی\", \"تھا\", \"ایک\", \"لیے\", \"نہیں\", \"برا\", \"اچھا\", \"تم\", \"کچھ\", \"کون\", \"کیا\"\n",
    "])\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in urdu_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 2: Handle Punctuation, Emojis, and Hashtags\n",
    "\n",
    "# Emoji dictionary to map sentiment\n",
    "emoji_sentiment = {\n",
    "    \"😂\": \"positive\", \"🤣\": \"positive\", \"😊\": \"positive\", \"😍\": \"positive\", \"😎\": \"positive\",\n",
    "    \"😢\": \"negative\", \"😠\": \"negative\", \"😡\": \"negative\", \"😭\": \"negative\", \"😒\": \"negative\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", '', text)\n",
    "    \n",
    "    # Replace emojis with sentiment\n",
    "    for emoji, sentiment in emoji_sentiment.items():\n",
    "        text = text.replace(emoji, sentiment)\n",
    "    \n",
    "    # Remove punctuation except Urdu script characters\n",
    "    urdu_punctuation = string.punctuation + \"؟\"\n",
    "    text = text.translate(str.maketrans('', '', urdu_punctuation))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 3: Filter short conversations (less than 3 words)\n",
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    return len(words) >= 3\n",
    "\n",
    "# Sample data to simulate your dataset (replace with your actual data)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        'یہ ایک مثال کا جملہ ہے 😊',\n",
    "        'کیا تم نے یہ سنا ہے؟',\n",
    "        'مجھے یہ برا لگا 😢',\n",
    "        'یہ ایک اور جملہ ہے #Hashtag',\n",
    "        'https://example.com یہ ایک لنک ہے'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame (replace this with pd.read_csv or your actual data loading method)\n",
    "urdu_data_cleaned = pd.DataFrame(data)\n",
    "\n",
    "# Apply the preprocessing steps to the dataset\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['urdu_text'].apply(preprocess_text)\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['preprocessed_text'].apply(remove_stopwords)\n",
    "urdu_data_cleaned = urdu_data_cleaned[urdu_data_cleaned['preprocessed_text'].apply(filter_short_posts)]\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "print(urdu_data_cleaned[['urdu_text', 'preprocessed_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400b8c4-a88d-4a17-a540-9868d32129e1",
   "metadata": {},
   "source": [
    "<h1> PHASE 2 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73bccaf-207c-424e-bfec-20c1f0217592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urdu_text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...</td>\n",
       "      <td>positivepositivepositive ہو لینے دے میری شادی ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...</td>\n",
       "      <td>چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "      <td>کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>`` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...</td>\n",
       "      <td>مراد علی شاہ کے بھیس ڈی جی آئی ایس آئی تھے حام...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥</td>\n",
       "      <td>قابل اعتبار ہی اکثر قاتل اعتبار ہوتے 💔🔥</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           urdu_text  \\\n",
       "0  🤣😂😂 ہو لینے دے میری شادی فسادن ٹھیک ہے کوجی نہ...   \n",
       "1  چل مہمانوں میں کھانا سرو کر چڑیل چاچی نوں دسدی...   \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
       "4   `` مراد علی شاہ کے بھیس میں ڈی جی آئی ایس آئی...   \n",
       "5        قابل اعتبار ہی اکثر قاتل اعتبار ہوتے ہیں 💔🔥   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  positivepositivepositive ہو لینے دے میری شادی ...  \n",
       "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں ...  \n",
       "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...  \n",
       "4  مراد علی شاہ کے بھیس ڈی جی آئی ایس آئی تھے حام...  \n",
       "5            قابل اعتبار ہی اکثر قاتل اعتبار ہوتے 💔🔥  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'urdu_sarcastic_dataset.csv'\n",
    "urdu_data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Define a list of Urdu stopwords\n",
    "urdu_stopwords = set([\n",
    "    \"اور\", \"یہ\", \"کہ\", \"کا\", \"کی\", \"ہے\", \"کو\", \"میں\", \"سے\", \"تھا\", \"ہوں\", \"ہیں\",\n",
    "    \"تھی\", \"تھا\", \"ایک\", \"لیے\", \"نہیں\", \"برا\", \"اچھا\", \"تم\", \"کچھ\", \"کون\", \"کیا\"\n",
    "])\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in urdu_stopwords]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Step 2: Handle Punctuation, Emojis, and Hashtags\n",
    "\n",
    "# Emoji dictionary to map sentiment\n",
    "emoji_sentiment = {\n",
    "    \"😂\": \"positive\", \"🤣\": \"positive\", \"😊\": \"positive\", \"😍\": \"positive\", \"😎\": \"positive\",\n",
    "    \"😢\": \"negative\", \"😠\": \"negative\", \"😡\": \"negative\", \"😭\": \"negative\", \"😒\": \"negative\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r\"#\\S+\", '', text)\n",
    "    \n",
    "    # Replace emojis with sentiment\n",
    "    for emoji, sentiment in emoji_sentiment.items():\n",
    "        text = text.replace(emoji, sentiment)\n",
    "    \n",
    "    # Remove punctuation except Urdu script characters\n",
    "    urdu_punctuation = string.punctuation + \"؟\"\n",
    "    text = text.translate(str.maketrans('', '', urdu_punctuation))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Step 3: Filter short conversations (less than 3 words)\n",
    "def filter_short_posts(text):\n",
    "    words = text.split()\n",
    "    return len(words) >= 3\n",
    "\n",
    "# Clean the dataset and apply preprocessing\n",
    "urdu_data_cleaned = urdu_data[['urdu_text', 'is_sarcastic']].dropna()\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['urdu_text'].apply(preprocess_text)\n",
    "urdu_data_cleaned['preprocessed_text'] = urdu_data_cleaned['preprocessed_text'].apply(remove_stopwords)\n",
    "urdu_data_cleaned = urdu_data_cleaned[urdu_data_cleaned['preprocessed_text'].apply(filter_short_posts)]\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "urdu_data_cleaned[['urdu_text', 'preprocessed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502795bb-1e9c-4f74-a40b-a723319f8be3",
   "metadata": {},
   "source": [
    "<h1> PHASE 3 </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94e995-42fc-4b6e-bdf4-2133746191dc",
   "metadata": {},
   "source": [
    "<h2> TOKENIZATION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e90c528-8603-4e36-8a05-a8dadcad7f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   preprocessed_text  \\\n",
      "0  positivepositivepositive ہو لینے دے میری شادی ...   \n",
      "1  چل مہمانوں کھانا سرو کر چڑیل چاچی نوں دسدی آں ...   \n",
      "2  کامران خان آپکی دن بھریہ زمہ داری لگائی گئی اپ...   \n",
      "4  مراد علی شاہ کے بھیس ڈی جی آئی ایس آئی تھے حام...   \n",
      "5            قابل اعتبار ہی اکثر قاتل اعتبار ہوتے 💔🔥   \n",
      "\n",
      "                                      tokenized_text  \n",
      "0  [positivepositivepositive, ہو, لینے, دے, میری,...  \n",
      "1  [چل, مہمانوں, کھانا, سرو, کر, چڑیل, چاچی, نوں,...  \n",
      "2  [کامران, خان, آپکی, دن, بھریہ, زمہ, داری, لگائ...  \n",
      "4  [مراد, علی, شاہ, کے, بھیس, ڈی, جی, آئی, ایس, آ...  \n",
      "5       [قابل, اعتبار, ہی, اکثر, قاتل, اعتبار, ہوتے]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenization function for Urdu text\n",
    "def urdu_tokenizer(text):\n",
    "    # Tokenize based on spaces and punctuation\n",
    "    tokens = re.findall(r'\\b[\\w؀-ۿ]+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to your dataset\n",
    "urdu_data_cleaned['tokenized_text'] = urdu_data_cleaned['preprocessed_text'].apply(urdu_tokenizer)\n",
    "\n",
    "# Display tokenized version of some Urdu social media posts\n",
    "print(urdu_data_cleaned[['preprocessed_text', 'tokenized_text']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4829461-d930-496c-9c5e-7497d6f3da02",
   "metadata": {},
   "source": [
    "<h1> TF-IDF </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a20781f-a889-4565-845f-8e2763e806c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  tfidf_score\n",
      "22153        کے   560.508533\n",
      "6956         تو   462.573622\n",
      "1447   positive   411.428428\n",
      "6139        بھی   408.940076\n",
      "17118        نے   366.229613\n",
      "20807        کر   343.570984\n",
      "23387        ہو   307.013534\n",
      "23660        ہی   283.271429\n",
      "18495        پر   283.140206\n",
      "2480         آپ   275.295158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a custom tokenizer using the previously defined function\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=urdu_tokenizer)\n",
    "\n",
    "# Fit and transform the data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(urdu_data_cleaned['preprocessed_text'])\n",
    "\n",
    "# Get feature names (i.e., the words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Calculate the TF-IDF scores\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "\n",
    "# Create a DataFrame with words and their TF-IDF scores\n",
    "tfidf_df = pd.DataFrame({'word': feature_names, 'tfidf_score': tfidf_scores})\n",
    "\n",
    "# Sort the DataFrame by TF-IDF scores in descending order\n",
    "tfidf_df = tfidf_df.sort_values(by='tfidf_score', ascending=False)\n",
    "\n",
    "# Display the top 10 words with the highest TF-IDF scores\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773185d1-b0b3-4e70-9081-1ceded2817cf",
   "metadata": {},
   "source": [
    "<h1> WORD2VEC </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01bdfe1-7090-4c15-892e-a4f5db2b149c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconll\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresources\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoNLL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\__init__.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding: utf8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"Pipeline module\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\core.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NormalizeParser\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizeParser\n\u001b[0;32m     10\u001b[0m NORMALIZE: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m TOKENIZE: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenize\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\pipeline\\parsers\\tokenize.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconll\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoNLL\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murduhack\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_tokenizer, word_tokenizer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parser\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenizeParser\u001b[39;00m(Parser):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\__init__.py:23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# coding: utf8\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mTokenization\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m==============\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03msentence and word tokenization.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sentence_tokenizer, word_tokenizer\n\u001b[0;32m     25\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\tokenizer.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _generate_sentences\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_model_exist, _preprocess_sentence, _retrieve_words, _load_model\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MODEL_PATH, VOCAB_PATH\n\u001b[0;32m     12\u001b[0m _WORD_TOKENIZER_MODEL, _CHAR2IDX, _IDX2CHAR \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urduhack\\tokenization\\keras_tokenizer.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndarray\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_vocab\u001b[39m(vocab_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming tokenized data is available in the column 'tokenized_text'\n",
    "# Tokenized data should be a list of lists, where each sublist is a tokenized sentence\n",
    "tokenized_data = urdu_data_cleaned['tokenized_text'].tolist()\n",
    "\n",
    "# Step 1: Train Word2Vec model on tokenized data\n",
    "word2vec_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 2: Find the top 5 words most similar to 'اچھا' (good)\n",
    "try:\n",
    "    similar_words = word2vec_model.wv.most_similar('اچھا', topn=5)\n",
    "    # Display the top 5 similar words\n",
    "    print(\"Top 5 words similar to 'اچھا':\", similar_words)\n",
    "except KeyError:\n",
    "    print(\"'اچھا' not found in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d32f6-3a1e-4d01-84b8-dac7b7ce7325",
   "metadata": {},
   "source": [
    "<h1> PHASE 4 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00285f70-ac5e-4f91-bcd2-aa64f1c3e159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c7f4fd-1d92-4c4e-9dd1-17751837a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Bigrams and their Frequencies:\n",
      "یہ ایک: 2\n",
      "ٹیسٹ پوسٹ: 2\n",
      "پوسٹ ہے: 2\n",
      "ایک ٹیسٹ: 1\n",
      "ایک اور: 1\n",
      "اور ٹیسٹ: 1\n",
      "ہم نگرام: 1\n",
      "نگرام کے: 1\n",
      "کے تجزیے: 1\n",
      "تجزیے کا: 1\n",
      "\n",
      "Top 10 Trigrams and their Frequencies:\n",
      "ٹیسٹ پوسٹ ہے: 2\n",
      "یہ ایک ٹیسٹ: 1\n",
      "ایک ٹیسٹ پوسٹ: 1\n",
      "یہ ایک اور: 1\n",
      "ایک اور ٹیسٹ: 1\n",
      "اور ٹیسٹ پوسٹ: 1\n",
      "ہم نگرام کے: 1\n",
      "نگرام کے تجزیے: 1\n",
      "کے تجزیے کا: 1\n",
      "تجزیے کا امتحان: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\intelcomputer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure that you have downloaded the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample dataset (Replace this with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"یہ ایک ٹیسٹ پوسٹ ہے\",\n",
    "        \"یہ ایک اور ٹیسٹ پوسٹ ہے\",\n",
    "        \"ہم نگرام کے تجزیے کا امتحان لے رہے ہیں\",\n",
    "        \"یہ تجزیہ اردو متن کے لیے ہے\"\n",
    "    ]\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Tokenization function for Urdu text\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# Create lists to store unigrams, bigrams, and trigrams\n",
    "unigrams = []\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "\n",
    "# Process each document\n",
    "for text in urdu_data['urdu_text']:\n",
    "    tokens = tokenize(text)\n",
    "    unigrams.extend(tokens)\n",
    "    bigrams.extend(list(ngrams(tokens, 2)))\n",
    "    trigrams.extend(list(ngrams(tokens, 3)))\n",
    "\n",
    "# Count frequencies of unigrams, bigrams, and trigrams\n",
    "unigram_freq = Counter(unigrams)\n",
    "bigram_freq = Counter(bigrams)\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# Get the top 10 most common bigrams and trigrams\n",
    "top_bigrams = bigram_freq.most_common(10)\n",
    "top_trigrams = trigram_freq.most_common(10)\n",
    "\n",
    "# Display the results\n",
    "print(\"Top 10 Bigrams and their Frequencies:\")\n",
    "for bigram, freq in top_bigrams:\n",
    "    print(f\"{' '.join(bigram)}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams and their Frequencies:\")\n",
    "for trigram, freq in top_trigrams:\n",
    "    print(f\"{' '.join(trigram)}: {freq}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3563100-d3c8-421d-9dd6-5a2079901fd3",
   "metadata": {},
   "source": [
    "<h1> PHASE 5 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "752f0d21-902d-4055-99de-2d53744df323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\intelcomputer\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn pandas nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f15e3f-1cea-4ce7-840a-6267661f879c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import nltk\n",
    "\n",
    "# Sample Urdu stopwords (You can expand this list)\n",
    "urdu_stopwords = set([\n",
    "    'کی', 'ہے', 'اور', 'میں', 'پر', 'نے', 'کو', 'کیوں', 'تھا', 'تھی', \n",
    "    'یہ', 'وہ', 'سب', 'کیا', 'جب', 'تک', 'ہم', 'میری', 'مجھے', 'ہوں'\n",
    "])\n",
    "\n",
    "# Sample dataset (Replace this with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"یہ ایک مثبت ٹیسٹ ہے\",\n",
    "        \"یہ منفی اثرات ہیں\",\n",
    "        \"میں بہت خوش ہوں\",\n",
    "        \"یہ بہت برا ہے\",\n",
    "        \"مجھے یہ پسند ہے\",\n",
    "        \"یہ بیکار ہے\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess text (remove stopwords)\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction using Tf-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Building - Using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "recall = recall_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "f1 = f1_score(y_test, y_pred, pos_label='positive', average='binary')\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef653924-ce9d-4124-8552-85a0c2b842d1",
   "metadata": {},
   "source": [
    "<h1> PHASE 6 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a595a6e4-aeda-4454-b85d-e274aee63b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 1.00\n",
      "F1 Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted  positive\n",
      "Actual             \n",
      "negative          1\n",
      "positive          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Sample Urdu dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"یہ ایک مثبت ٹیسٹ ہے\",\n",
    "        \"یہ منفی اثرات ہیں\",\n",
    "        \"میں بہت خوش ہوں\",\n",
    "        \"یہ بہت برا ہے\",\n",
    "        \"مجھے یہ پسند ہے\",\n",
    "        \"یہ بیکار ہے\",\n",
    "        \"یہ ایک حیرت انگیز کام ہے\",\n",
    "        \"یہ واقعی مضحکہ خیز ہے\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n",
    "\n",
    "# Preprocess text (manual stopwords as before)\n",
    "urdu_stopwords = set([...])  # Fill in with the previously defined stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Clean the text\n",
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n",
    "\n",
    "# Feature extraction using Tf-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n",
    "\n",
    "# Model Building - Using Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_recall = recall_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_f1 = f1_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "\n",
    "# Display the evaluation metrics for validation\n",
    "print(\"Validation Set Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Precision: {val_precision:.2f}\")\n",
    "print(f\"Recall: {val_recall:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Analyze predictions\n",
    "confusion_matrix = pd.crosstab(y_val, y_val_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdf0873-deb2-46cd-ba67-432defe6ee9c",
   "metadata": {},
   "source": [
    "<h1> DESCRIPTION </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbbe887f-15c4-4849-bc78-e5ddc709aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f2dc9c-b9f7-43a3-9757-f0ebbe62e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'urdu_text': [\n",
    "        \"یہ ایک مثبت ٹیسٹ ہے\",\n",
    "        \"یہ منفی اثرات ہیں\",\n",
    "        \"میں بہت خوش ہوں\",\n",
    "        \"یہ بہت برا ہے\",\n",
    "        \"مجھے یہ پسند ہے\",\n",
    "        \"یہ بیکار ہے\",\n",
    "        \"یہ ایک حیرت انگیز کام ہے\",\n",
    "        \"یہ واقعی مضحکہ خیز ہے\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "urdu_data = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a49bb5ac-0325-4cdb-b251-534de264c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_stopwords = set([...])  # Fill in with the previously defined stopwords\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in urdu_stopwords]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed881665-6074-4128-af3e-0b2750e8c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urdu_data['cleaned_text'] = urdu_data['urdu_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06a37e12-25e2-417a-8d19-c070623b52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(urdu_data['cleaned_text'])\n",
    "y = urdu_data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69e07c72-8782-4f7a-b4b2-0fdd443ebedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eadd58b0-d473-4047-9360-9739d4404441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e67fc266-a38a-4569-8755-2a5e1254d3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set Evaluation Metrics:\n",
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 1.00\n",
      "F1 Score: 0.67\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         1\n",
      "    positive       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\intelcomputer\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_recall = recall_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "val_f1 = f1_score(y_val, y_val_pred, pos_label='positive', average='binary')\n",
    "\n",
    "print(\"Validation Set Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Precision: {val_precision:.2f}\")\n",
    "print(f\"Recall: {val_recall:.2f}\")\n",
    "print(f\"F1 Score: {val_f1:.2f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3145e065-e7d0-410e-8180-661b3893dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "Predicted  positive\n",
      "Actual             \n",
      "negative          1\n",
      "positive          1\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = pd.crosstab(y_val, y_val_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1939209-ffc7-4e9b-92bd-9809171572ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
